sources:
  fluentbit:
    type: socket
    address: 0.0.0.0:24224
    mode: tcp
    format: fluent

transforms:
  # Séparation initiale des logs et des métriques
  data_router:
    type: remap
    inputs:
      - fluentbit
    source: |
      log("Événement reçu: " + to_string(.) ?? "impossible à convertir en string", level: "debug")

      if !exists(.tag) {
        .tag = "unknown"
      } else if !is_string(.tag) {
        .tag = to_string(.tag) ?? "unknown"
      }

      if exists(.message) && (is_object(.message) || is_string(.message)) {
        message_obj = .message

        if is_string(message_obj) {
          parsed = parse_json(message_obj) ?? null
          if parsed != null {
            message_obj = parsed
          }
        }

        if is_array(message_obj) && array_length(message_obj) > 0 {
          message_obj = message_obj[0]

          if is_string(message_obj) {
            parsed = parse_json(message_obj) ?? null
            if parsed != null {
              message_obj = parsed
            }
          }
        }

        if is_object(message_obj) && (exists(message_obj.cpu_p) || exists(message_obj["Mem.used"]) || exists(message_obj.read_size)) {
          .event_type = "metric"
          .content = message_obj
        } else if is_object(message_obj) && exists(message_obj.log) {
          .event_type = "log"
          .content = message_obj
        } else {
          .event_type = "log"
          .content = message_obj
        }
      } else {
        .event_type = "unknown"
        log("Type d'événement inconnu ou format non reconnu: " + to_string(.tag), level: "warn")
        null
      }

  process_metrics:
    type: remap
    inputs:
      - data_router
    source: |
      if .event_type != "metric" {
        null
      }

      content = .content

      .timestamp = now()
      if exists(content.date) {
        ts_float, err = to_float(content.date)
        if err == null {
          ts_ms = if ts_float < 1000000000000.0 {
            to_int(ts_float * 1000.0)
          } else {
            to_int(ts_float)
          }
          if ts_ms != null {
            .timestamp = from_unix_timestamp(ts_ms, unit: "milliseconds") ?? now()
          }
        }
      }

      if exists(content.host) {
        .host = to_string(content.host) ?? "unknown"
      }

      .service = "system"

      .tags = {}
      if exists(content.cpu_p) {
        .metric_name = "cpu"
        .metric_value = to_float(content.cpu_p) ?? 0.0
        .tags.user_p = to_string(content.user_p) ?? ""
        .tags.system_p = to_string(content.system_p) ?? ""
      } else if exists(content."Mem.used") {
        .metric_name = "mem"
        .metric_value = to_float(content."Mem.used") ?? 0.0
        .tags.total = to_string(content."Mem.total") ?? ""
        .tags.free = to_string(content."Mem.free") ?? ""
      } else if exists(content.read_size) || exists(content.write_size) {
        .metric_name = "disk"
        .metric_value = to_float(content.write_size) ?? 0.0
        .tags.read_size = to_string(content.read_size) ?? "0"
      } else {
        log("Type de métrique non reconnu: " + to_string(content) ?? "", level: "warn")
        .metric_name = "unknown"
        .metric_value = 0.0
      }

      msg = "Métrique traitée: " + to_string(.metric_name) + " = " + to_string(.metric_value)
      log(msg, level: "debug")

  process_logs:
    type: remap
    inputs:
      - data_router
    source: |
      if .event_type != "log" {
        null
      }

      content = .content
      .timestamp = now()

      if exists(content.date) {
        ts_float, err = to_float(content.date)
        if err == null {
          ts_ms = if ts_float < 1000000000000.0 {
            to_int(ts_float * 1000.0)
          } else {
            to_int(ts_float)
          }
          if ts_ms != null {
            .timestamp = from_unix_timestamp(ts_ms, unit: "milliseconds") ?? now()
          }
        }
      }

      if exists(content.host) {
        .host = to_string(content.host) ?? "unknown"
      }

      .service = "app"
      .tags = {}

      if exists(content.log) {
        .message = to_string(content.log) ?? ""

        if contains(.message, " ERROR ") || contains(.message, "[ERROR]") {
          .level = "error"
        } else if contains(.message, " WARN ") || contains(.message, "[WARN]") {
          .level = "warn"
        } else if contains(.message, " INFO ") || contains(.message, "[INFO]") {
          .level = "info"
        } else if contains(.message, " DEBUG ") || contains(.message, "[DEBUG]") {
          .level = "debug"
        } else {
          .level = "info"
        }

        if contains(.message, "{\"") {
          json_start = index(.message, "{\"") ?? -1
          if json_start >= 0 {
            json_part = substring(.message, json_start)
            parsed_log = parse_json(json_part) ?? null
            if parsed_log != null {
              if exists(parsed_log.level) {
                .level = parsed_log.level
              }
              if exists(parsed_log.caller) {
                .tags.caller = parsed_log.caller
              }
              if exists(parsed_log.event) {
                .tags.event = parsed_log.event
              }
            }
          }
        }

      } else {
        .message = to_string(content) ?? "{}"
        .level = "info"
      }

      preview = substring(.message, 0, 50) ?? ""
      log("Log traité de niveau " + to_string(.level) + ": " + preview + "...", level: "debug")

sinks:
  metrics_clickhouse:
    type: clickhouse
    inputs:
      - process_metrics
    endpoint: http://89.116.38.238:8123
    database: default
    table: metrics_log
    compression: gzip
    skip_unknown_fields: true
    buffer:
      type: memory
      max_events: 100
      when_full: drop_newest
    batch:
      max_events: 10
      timeout_secs: 1
    encoding:
      only_fields:
        - timestamp
        - host
        - service
        - metric_name
        - metric_value
        - tags
    request:
      timeout_secs: 5
      retry_attempts: 3

  logs_clickhouse:
    type: clickhouse
    inputs:
      - process_logs
    endpoint: http://89.116.38.238:8123
    database: default
    table: logs_text
    compression: gzip
    skip_unknown_fields: true
    buffer:
      type: memory
      max_events: 100
      when_full: drop_newest
    batch:
      max_events: 10
      timeout_secs: 1
    encoding:
      only_fields:
        - timestamp
        - host
        - service
        - level
        - message
        - tags
    request:
      timeout_secs: 5
      retry_attempts: 3

  debug_console:
    type: console
    inputs:
      - process_metrics
      - process_logs
    encoding:
      codec: json
