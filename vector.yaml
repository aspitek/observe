sources:
  fluentbit:
    type: socket
    address: 0.0.0.0:24224
    mode: tcp
    format: fluent

transforms:
  metrics_processing:
    type: remap
    inputs:
      - fluentbit
    source: |
      # Ensure .tag is a string with a default fallback
      .tag = to_string(.tag) ?? "unknown"
      # Strict filter for metrics
      if !starts_with(.tag, "metrics.") {
        null
      } else {
        # Set host and service
        .host = to_string(.host) ?? "unknown"
        .service = "system"
        .metric_name = replace(.tag, "metrics.", "")

        # Parse message (Fluent Forward may wrap entries)
        message = .message
        if is_array(.message) {
          # Handle array of entries
          message = .message[0]
        }
        message_str, err = to_string(message)
        if err != null {
          log("Failed to convert .message to string: " + to_string(err), level: "warn")
          .metric_value = 0.0
          .timestamp = now()
          .tags = {}
        } else {
          parsed, err = parse_json(message_str)
          if err != null {
            log("Failed to parse JSON in metrics event: " + to_string(err), level: "warn")
            .metric_value = 0.0
            .timestamp = now()
            .tags = {}
          } else {
            # Extract metric value
            .metric_value = if .metric_name == "cpu" {
              to_float(parsed.cpu_p) ?? 0.0
            } else if .metric_name == "mem" {
              to_float(parsed."Mem.used") ?? 0.0
            } else if .metric_name == "disk" {
              to_float(parsed.write_size) ?? 0.0
            } else {
              0.0
            }

            # Handle timestamp
            date_val, err = to_float(parsed.date)
            if err != null {
              date_val = to_float(now()) ?? 0.0
            }
            date_ms = date_val * 1000
            if date_ms < 0 || is_nan(date_ms) {
              log("Invalid timestamp in metrics event: " + to_string(date_ms), level: "warn")
              date_ms = to_float(now()) ?? 0.0
            }
            .timestamp, err = from_unix_timestamp(to_int(date_ms))
            if err != null {
              log("Failed to convert timestamp: " + to_string(err), level: "warn")
              .timestamp = now()
            }

            if date_ms < 0 || is_nan(date_ms) {
              log("Invalid timestamp in metrics event: " + to_string(date_ms), level: "warn")
              date_ms = to_float(now())
            }
            .timestamp, err = from_unix_timestamp(to_int(date_ms))
            if err != null {
              log("Failed to convert timestamp: " + to_string(err), level: "warn")
              .timestamp = now()
            }

            # Extract tags
            .tags = if .metric_name == "cpu" && parsed.cpu0 != null {
              {
                "cpu_core": "0",
                "p_user": to_string(parsed.cpu0.p_user) ?? "",
                "p_system": to_string(parsed.cpu0.p_system) ?? ""
              }
            } else {
              {}
            }
          }
        }
      }

  logs_processing:
    type: remap
    inputs:
      - fluentbit
    source: |
      # Ensure .tag is a string with a default fallback
      .tag = to_string(.tag) ?? "unknown"
      # Strict filter for logs
      if !starts_with(.tag, "logs.") {
        null
      } else {
        .host = to_string(.host) ?? "unknown"
        .service = "app"

        # Handle Fluent Forward message
        message = .message
        if !is_array(message) {
          message = [message]
        }

        # Process each entry
        entry = if is_array(.message) { .message[0] } else { .message }
        entry_str, err = to_string(entry)
        if err != null {
          log("Failed to convert entry to string: " + to_string(err), level: "warn")
          .level = "unknown"
          .message = "Invalid entry format"
          .tags = {}
          .timestamp = now()
        } else {
          parsed, err = parse_json(entry_str)
          if err != null {
            log("Failed to parse JSON in log entry: " + to_string(err), level: "warn")
            .level = "unknown"
            .message = entry_str
            .tags = {}
            .timestamp = now()
          } else {
            .tags = parsed.tags ?? {}
            .level = to_string(parsed.level) ?? "unknown"
            .message = to_string(parsed.log) ?? ""

            date_val, err = to_float(parsed.date)
            if err != null {
              date_val = to_float(now()) ?? 0.0
            }
            date_ms = date_val * 1000
            if date_ms < 0 || is_nan(date_ms) {
              log("Invalid timestamp in log entry: " + to_string(date_ms), level: "warn")
              .timestamp = now()
            } else {
              .timestamp, err = from_unix_timestamp(to_int(date_ms))
              if err != null {
                log("Failed to convert timestamp: " + to_string(err), level: "warn")
                .timestamp = now()
              }
            }
          }
        }

      }

sinks:
  metrics_clickhouse:
    type: clickhouse
    inputs:
      - metrics_processing
    endpoint: http://89.116.38.238:8123
    database: default
    table: metrics_log
    compression: gzip
    skip_unknown_fields: true
    date_time_best_effort: true
    buffer:
      type: memory
      max_events: 100
      when_full: drop_newest
    batch:
      max_events: 5
      timeout_secs: 0.2
    encoding:
      only_fields:
        - timestamp
        - host
        - service
        - metric_name
        - metric_value
        - tags
    request:
      timeout_secs: 3
      retry_attempts: 5

  logs_clickhouse:
    type: clickhouse
    inputs:
      - logs_processing
    endpoint: http://89.116.38.238:8123
    database: default
    table: logs_text
    compression: gzip
    skip_unknown_fields: true
    date_time_best_effort: true
    buffer:
      type: memory
      max_events: 100
      when_full: drop_newest
    batch:
      max_events: 5
      timeout_secs: 0.2
    encoding:
      only_fields:
        - timestamp
        - host
        - service
        - level
        - message
        - tags
    request:
      timeout_secs: 3
      retry_attempts: 5

  debug_console:
    type: console
    inputs:
      - metrics_processing
      - logs_processing
    encoding:
      codec: json