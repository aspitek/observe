sources:
  fluentbit:
    type: socket
    address: 0.0.0.0:24224
    mode: tcp
    # Explicitly set Fluent Forward protocol
    format: fluent

transforms:
  metrics_processing:
    type: remap
    inputs:
      - fluentbit
    source: |
      # Strict filter for metrics
      if !starts_with(.tag, "metrics.") {
        null
      } else {
        # Ensure tag is a string
        .tag, err = to_string(.tag)
        if err != null {
          log("Failed to convert .tag to string: " + to_string(err), level: "warn")
          .tag = "unknown"
        }
        # Set host and service
        .host = to_string(.host) ?? "unknown"
        .service = "system"
        .metric_name = replace(.tag, "metrics.", "")

        # Parse message (Fluent Forward may wrap entries)
        message = .message
        if is_array(.message) {
          # Handle case where message is an array of entries
          message = .message[0]
        }
        message_str, err = to_string(message)
        if err != null {
          log("Failed to convert .message to string: " + to_string(err), level: "warn")
          .metric_value = 0.0
          .timestamp = now()
          .tags = {}
        } else {
          parsed, err = parse_json(message_str)
          if err != null {
            log("Failed to parse JSON in metrics event: " + to_string(err), level: "warn")
            .metric_value = 0.0
            .timestamp = now()
            .tags = {}
          } else {
            # Extract metric value
            .metric_value = if .metric_name == "cpu" {
              to_float(parsed.cpu_p) ?? 0.0
            } else if .metric_name == "mem" {
              to_float(parsed."Mem.used") ?? 0.0
            } else if .metric_name == "disk" {
              to_float(parsed.write_size) ?? 0.0
            } else {
              0.0
            }

            # Handle timestamp
            date_ms = if parsed.date != null {
              to_float(parsed.date) * 1000
            } else {
              to_float(now())
            }
            if date_ms < 0 || is_nan(date_ms) {
              log("Invalid timestamp in metrics event: " + to_string(date_ms), level: "warn")
              date_ms = to_float(now())
            }
            .timestamp, err = from_unix_timestamp(to_int(date_ms))
            if err != null {
              log("Failed to convert timestamp: " + to_string(err), level: "warn")
              .timestamp = now()
            }

            # Extract tags
            .tags = if .metric_name == "cpu" && parsed.cpu0 != null {
              {
                "cpu_core": "0",
                "p_user": to_string(parsed.cpu0.p_user) ?? "",
                "p_system": to_string(parsed.cpu0.p_system) ?? ""
              }
            } else {
              {}
            }
          }
        }
      }

  logs_processing:
    type: remap
    inputs:
      - fluentbit
    source: |
      # Strict filter for logs
      if !starts_with(.tag, "logs.") {
        null
      } else {
        .tag, err = to_string(.tag)
        if err != null {
          log("Failed to convert .tag to string: " + to_string(err), level: "warn")
          .tag = "logs.unknown"
        }
        .host = to_string(.host) ?? "unknown"
        .service = "app"

        # Handle Fluent Forward message (may be array or object)
        message = .message
        if !is_array(message) {
          # Wrap single message in array for consistent processing
          message = [message]
        }

        # Process each entry
        for_each(array!(message)) -> |index, entry| {
          entry_str, err = to_string(entry)
          if err != null {
            log("Failed to convert entry to string: " + to_string(err), level: "warn")
            event = {
              "host": .host,
              "service": .service,
              "tags": {},
              "level": "unknown",
              "message": "Invalid entry format",
              "timestamp": now()
            }
            emit!(event)
          } else {
            parsed, err = parse_json(entry_str)
            if err != null {
              log("Failed to parse JSON in log entry: " + to_string(err), level: "warn")
              event = {
                "host": .host,
                "service": .service,
                "tags": {},
                "level": "unknown",
                "message": entry_str,
                "timestamp": now()
              }
              emit!(event)
            } else {
              event = {
                "host": .host,
                "service": .service,
                "tags": parsed.tags ?? {},
                "level": to_string(parsed.level) ?? "unknown",
                "message": to_string(parsed.log) ?? ""
              }
              # Handle timestamp
              date_ms = if parsed.date != null {
                to_float(parsed.date) * 1000
              } else {
                to_float(now())
              }
              if date_ms < 0 || is_nan(date_ms) {
                log("Invalid timestamp in log entry: " + to_string(date_ms), level: "warn")
                event.timestamp = now()
              } else {
                event.timestamp, err = from_unix_timestamp(to_int(date_ms))
                if err != null {
                  log("Failed to convert timestamp: " + to_string(err), level: "warn")
                  event.timestamp = now()
                }
              }
              emit!(event)
            }
          }
        }
        # Drop original event
        null
      }

sinks:
  metrics_clickhouse:
    type: clickhouse
    inputs:
      - metrics_processing
    endpoint: http://89.116.38.238:8123
    database: default
    table: metrics_log
    compression: gzip
    skip_unknown_fields: true
    date_time_best_effort: true
    buffer:
      type: memory
      max_events: 100
      when_full: drop_newest
    batch:
      max_events: 5
      timeout_secs: 0.2
    encoding:
      only_fields:
        - timestamp
        - host
        - service
        - metric_name
        - metric_value
        - tags
    request:
      timeout_secs: 3
      retry_attempts: 5

  logs_clickhouse:
    type: clickhouse
    inputs:
      - logs_processing
    endpoint: http://89.116.38.238:8123
    database: default
    table: logs_text
    compression: gzip
    skip_unknown_fields: true
    date_time_best_effort: true
    buffer:
      type: memory
      max_events: 100
      when_full: drop_newest
    batch:
      max_events: 5
      timeout_secs: 0.2
    encoding:
      only_fields:
        - timestamp
        - host
        - service
        - level
        - message
        - tags
    request:
      timeout_secs: 3
      retry_attempts: 5

  debug_console:
    type: console
    inputs:
      - metrics_processing
      - logs_processing
    encoding:
      codec: json